{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to build and utilize a predictive model for forecasting player performance in upcoming game weeks (GWs) for different positions in soccer, such as Goalkeepers (GK), Defenders (DEF), Midfielders (MID), and Forwards (FWD).\n",
    "\n",
    "Here's the workflow:\n",
    "\n",
    "1. Training Phase:\n",
    "\n",
    "- Master CSV Files: The model is trained on master CSV files (e.g., \"GK_master.csv\") containing historical data for each position.\n",
    "- Data Preprocessing: The data is preprocessed, including normalization, to create training and test sets.\n",
    "- Model Training: A neural network model is trained for each position, using the respective training sets.\n",
    "\n",
    "2. Prediction Phase:\n",
    "\n",
    "- Previous GW Data: Before making predictions for a new GW, the data for the previous GW (e.g., \"GK_data.csv\") is loaded.\n",
    "- Making Predictions: The trained model makes predictions for the top-performing players for the upcoming GW.\n",
    "- Saving Predictions: The predictions are saved into new CSV files (e.g., \"GK_prediction.csv\"), containing the names and predicted points of the top players.\n",
    "\n",
    "By following this workflow, the notebook produces actionable insights for each player position, aiding in decision-making for fantasy soccer or other related applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "206/206 [==============================] - 1s 2ms/step - loss: 3.4686 - val_loss: 2.6435\n",
      "Epoch 2/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8904 - val_loss: 2.6906\n",
      "Epoch 3/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8673 - val_loss: 2.6872\n",
      "Epoch 4/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8748 - val_loss: 2.7140\n",
      "Epoch 5/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8576 - val_loss: 2.6479\n",
      "Epoch 6/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8500 - val_loss: 2.6574\n",
      "Epoch 7/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8462 - val_loss: 2.6707\n",
      "Epoch 8/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8402 - val_loss: 2.6572\n",
      "Epoch 9/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8342 - val_loss: 2.6576\n",
      "Epoch 10/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8309 - val_loss: 2.6257\n",
      "Epoch 11/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8367 - val_loss: 2.6574\n",
      "Epoch 12/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8225 - val_loss: 2.6584\n",
      "Epoch 13/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8234 - val_loss: 2.6712\n",
      "Epoch 14/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8261 - val_loss: 2.6671\n",
      "Epoch 15/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8167 - val_loss: 2.6580\n",
      "Epoch 16/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8123 - val_loss: 2.7266\n",
      "Epoch 17/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8231 - val_loss: 2.6713\n",
      "Epoch 18/20\n",
      "206/206 [==============================] - 0s 2ms/step - loss: 2.8188 - val_loss: 2.6804\n",
      "Epoch 19/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8174 - val_loss: 2.6696\n",
      "Epoch 20/20\n",
      "206/206 [==============================] - 0s 1ms/step - loss: 2.8248 - val_loss: 2.6868\n"
     ]
    }
   ],
   "source": [
    "# Load the combined CSV file into a DataFrame\n",
    "df = pd.read_csv('GK_master.csv')\n",
    "\n",
    "# Create a new column for next week's total points\n",
    "df['next_week_total_points'] = df.groupby('name')['total_points'].shift(-1)\n",
    "\n",
    "# Drop rows with NaNs caused by the shift operation\n",
    "df.dropna(subset=['next_week_total_points'], inplace=True)\n",
    "\n",
    "# Split the DataFrame into features (X) and target (y)\n",
    "X = df.drop(['name', 'element', 'next_week_total_points'], axis=1)\n",
    "y = df['next_week_total_points']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GK_24.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-73b2a0e9bebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the data for previous gameweek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_24\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GK_24.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Prepare the feature data. Drop the columns that were dropped during training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_24\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_24\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'element'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GK_24.csv'"
     ]
    }
   ],
   "source": [
    "# Load the data for previous gameweek\n",
    "df_24 = pd.read_csv('GK_data.csv')\n",
    "\n",
    "# Prepare the feature data. Drop the columns that were dropped during training.\n",
    "X_24 = df_24.drop(['name', 'element'], axis=1)\n",
    "\n",
    "# Scale the features using the same scaler used in training.\n",
    "X_24_scaled = scaler.transform(X_24)\n",
    "\n",
    "# Make predictions using the trained model.\n",
    "predictions = model.predict(X_24_scaled)\n",
    "\n",
    "# Add the predictions to the DataFrame.\n",
    "df_24['predicted_points'] = predictions\n",
    "\n",
    "# Sort the DataFrame by the predicted points in descending order and take the top 10 players.\n",
    "top_players = df_24.sort_values('predicted_points', ascending=False).head(10)\n",
    "\n",
    "# Save the top players DataFrame.\n",
    "top_players[['name', 'predicted_points']].to_csv('GK_prediction.csv', index=False)\n",
    "\n",
    "# Print the top players DataFrame.\n",
    "print(top_players[['name', 'predicted_points']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined CSV file into a DataFrame\n",
    "df = pd.read_csv('DEF_master.csv')\n",
    "\n",
    "\n",
    "# Create a new column for next week's total points\n",
    "df['next_week_total_points'] = df.groupby('name')['total_points'].shift(-1)\n",
    "\n",
    "# Drop rows with NaNs caused by the shift operation\n",
    "df.dropna(subset=['next_week_total_points'], inplace=True)\n",
    "\n",
    "# Split the DataFrame into features (X) and target (y)\n",
    "X = df.drop(['name', 'element', 'next_week_total_points'], axis=1)\n",
    "y = df['next_week_total_points']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for previous gameweek\n",
    "df_24 = pd.read_csv('DEF_data.csv')\n",
    "\n",
    "# Prepare the feature data. Drop the columns that were dropped during training.\n",
    "X_24 = df_24.drop(['name', 'element'], axis=1)\n",
    "\n",
    "# Scale the features using the same scaler used in training.\n",
    "X_24_scaled = scaler.transform(X_24)\n",
    "\n",
    "# Make predictions using the trained model.\n",
    "predictions = model.predict(X_24_scaled)\n",
    "\n",
    "# Add the predictions to the DataFrame.\n",
    "df_24['predicted_points'] = predictions\n",
    "\n",
    "# Sort the DataFrame by the predicted points in descending order and take the top 10 players.\n",
    "top_players = df_24.sort_values('predicted_points', ascending=False).head(10)\n",
    "\n",
    "# Save the top players DataFrame.\n",
    "top_players[['name', 'predicted_points']].to_csv('DEF_prediction.csv', index=False)\n",
    "\n",
    "# Print the top players DataFrame.\n",
    "print(top_players[['name', 'predicted_points']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined CSV file into a DataFrame\n",
    "df = pd.read_csv('MID_master.csv')\n",
    "\n",
    "\n",
    "# Create a new column for next week's total points\n",
    "df['next_week_total_points'] = df.groupby('name')['total_points'].shift(-1)\n",
    "\n",
    "# Drop rows with NaNs caused by the shift operation\n",
    "df.dropna(subset=['next_week_total_points'], inplace=True)\n",
    "\n",
    "# Split the DataFrame into features (X) and target (y)\n",
    "X = df.drop(['name', 'element', 'next_week_total_points'], axis=1)\n",
    "y = df['next_week_total_points']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for previous gameweek\n",
    "df_24 = pd.read_csv('MID_data.csv')\n",
    "\n",
    "# Prepare the feature data. Drop the columns that were dropped during training.\n",
    "X_24 = df_24.drop(['name', 'element'], axis=1)\n",
    "\n",
    "# Scale the features using the same scaler used in training.\n",
    "X_24_scaled = scaler.transform(X_24)\n",
    "\n",
    "# Make predictions using the trained model.\n",
    "predictions = model.predict(X_24_scaled)\n",
    "\n",
    "# Add the predictions to the DataFrame.\n",
    "df_24['predicted_points'] = predictions\n",
    "\n",
    "# Sort the DataFrame by the predicted points in descending order and take the top 10 players.\n",
    "top_players = df_24.sort_values('predicted_points', ascending=False).head(10)\n",
    "\n",
    "# Save the top players DataFrame.\n",
    "top_players[['name', 'predicted_points']].to_csv('MID_prediction.csv', index=False)\n",
    "\n",
    "# Print the top players DataFrame.\n",
    "print(top_players[['name', 'predicted_points']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined CSV file into a DataFrame\n",
    "df = pd.read_csv('FWD_master.csv')\n",
    "\n",
    "\n",
    "# Create a new column for next week's total points\n",
    "df['next_week_total_points'] = df.groupby('name')['total_points'].shift(-1)\n",
    "\n",
    "# Drop rows with NaNs caused by the shift operation\n",
    "df.dropna(subset=['next_week_total_points'], inplace=True)\n",
    "\n",
    "# Split the DataFrame into features (X) and target (y)\n",
    "X = df.drop(['name', 'element', 'next_week_total_points'], axis=1)\n",
    "y = df['next_week_total_points']\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for previous gameweek\n",
    "df_24 = pd.read_csv('FWD_data.csv')\n",
    "\n",
    "# Prepare the feature data. Drop the columns that were dropped during training.\n",
    "X_24 = df_24.drop(['name', 'element'], axis=1)\n",
    "\n",
    "# Scale the features using the same scaler used in training.\n",
    "X_24_scaled = scaler.transform(X_24)\n",
    "\n",
    "# Make predictions using the trained model.\n",
    "predictions = model.predict(X_24_scaled)\n",
    "\n",
    "# Add the predictions to the DataFrame.\n",
    "df_24['predicted_points'] = predictions\n",
    "\n",
    "# Sort the DataFrame by the predicted points in descending order and take the top 10 players.\n",
    "top_players = df_24.sort_values('predicted_points', ascending=False).head(10)\n",
    "\n",
    "# Save the top players DataFrame.\n",
    "top_players[['name', 'predicted_points']].to_csv('FWD_prediction.csv', index=False)\n",
    "\n",
    "# Print the top players DataFrame.\n",
    "print(top_players[['name', 'predicted_points']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the positions and corresponding prediction files\n",
    "positions = ['GK', 'DEF', 'MID', 'FWD']\n",
    "prediction_files = {'GK': 'GK_prediction.csv', 'DEF': 'DEF_prediction.csv', 'MID': 'MID_prediction.csv', 'FWD': 'FWD_prediction.csv'}\n",
    "\n",
    "# Create a DataFrame to store the top 10 players for each position\n",
    "top_players_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over the positions and read the corresponding prediction file\n",
    "for position in positions:\n",
    "    prediction_df = pd.read_csv(prediction_files[position])\n",
    "    top_players = prediction_df['name'].head(10).tolist()  # Assuming the 'name' column contains player names\n",
    "    top_players_df[position] = top_players\n",
    "\n",
    "# Save the DataFrame to a TXT file with the game week in the title\n",
    "game_week = 2  # Replace with your game week\n",
    "filename = f'Gameweek_{game_week}_predictions.txt'\n",
    "top_players_df.to_csv(filename, index=False, sep='\\t')\n",
    "\n",
    "print(f\"Top players for Gameweek {game_week} saved to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
